import onnx
from onnx import helper, TensorProto

# Importations QONNX
from qonnx.core.modelwrapper import ModelWrapper
from qonnx.transformation.channels_last import ConvertToChannelsLastAndClean
from qonnx.util.cleanup import cleanup_model  # <- L'outil de nettoyage officiel
from qonnx.transformation.gemm_to_matmul import GemmToMatMul

print("--- Ã‰TAPE A : CHARGEMENT ET NETTOYAGE (via QONNX) ---")

# === 1. Charger le modÃ¨le ORIGINAL ===
onnx_model_path = "models/cnn_gesture_brevitas_int8.onnx"
print(f"ðŸ” Chargement du modÃ¨le ONNX : {onnx_model_path}")
onnx_model = onnx.load(onnx_model_path)


# === 2. ForÃ§age de la forme d'entrÃ©e (NCHW) ===
# Le nettoyage a besoin de connaÃ®tre la forme d'entrÃ©e
input_name = "inp.1"
input_shape = [1, 1, 64, 64] # Forme NCHW originale

input_tensor = next((x for x in onnx_model.graph.input if x.name == input_name), None)
if input_tensor is None:
    print(f"âš ï¸ EntrÃ©e {input_name} non trouvÃ©e, ajout manuel.")
    input_tensor = helper.make_tensor_value_info(input_name, TensorProto.FLOAT, input_shape)
    onnx_model.graph.input.append(input_tensor)
else:
    print(f"âœ… EntrÃ©e {input_name} trouvÃ©e. ForÃ§age de la forme Ã  {input_shape}.")
    input_tensor.type.tensor_type.ClearField('shape')
    input_tensor.type.tensor_type.elem_type = TensorProto.FLOAT
    for dim_val in input_shape:
         input_tensor.type.tensor_type.shape.dim.extend([
             onnx.TensorShapeProto.Dimension(dim_value=dim_val)
         ])

# === 3. Nettoyage et InfÃ©rence de Forme QONNX ===
print("ðŸ§¹ Nettoyage du modÃ¨le avec qonnx.util.cleanup.cleanup_model...")
# Emballer le modÃ¨le
model_to_clean = ModelWrapper(onnx_model)

# Appliquer le nettoyage. 
# Ceci va (correctement) :
#  - Retirer les initializers des entrÃ©es
#  - ExÃ©cuter une infÃ©rence de forme robuste
#  - Propager les formes Ã  travers tout le graphe
cleaned_model = cleanup_model(model_to_clean)
print("âœ… ModÃ¨le NCHW nettoyÃ© et formes infÃ©rÃ©es.")


print("ðŸ”„ Conversion de Gemm en MatMul...")
cleaned_model = cleaned_model.transform(GemmToMatMul())
print("âœ… Transformation GemmToMatMul rÃ©ussie.")


print("\n--- Ã‰TAPE B : CONVERSION VERS CHANNELS-LAST (NHWC) ---")

# === 4. Conversion en "channels-last" ===
try:
    # Nous avons dÃ©jÃ  un ModelWrapper "cleaned_model"
    print("ðŸ”„ Application de la transformation 'ConvertToChannelsLastAndClean'...")
    model_channels_last = cleaned_model.transform(
        ConvertToChannelsLastAndClean(make_input_channels_last=True)
    )
    
    print("âœ… Transformation rÃ©ussie.")

    print("âœï¸  VÃ©rification finale des noms de nÅ“uds (post-transformation)...")
    unnamed_count = 0
    final_graph = model_channels_last.model.graph
    output_tensor_name = "global_out"
    output_node_found = False

    for i, node in enumerate(final_graph.node):
        # 1. Renommer les nÅ“uds vides
        if not node.name: 
            node.name = f"{node.op_type}_{i}_unnamed_post_FIXED"
            unnamed_count += 1

        # 2. Trouver et nommer la couche de sortie finale
        if output_tensor_name in node.output:
            print(f"âœ… NÅ“ud de sortie (type {node.op_type}) trouvÃ©. Renommage en 'final_output_layer'.")
            node.name = "final_output_layer"
            output_node_found = True

    if not output_node_found:
        print(f"âš ï¸ ATTENTION: Impossible de trouver le nÅ“ud qui produit '{output_tensor_name}'!")

    print(f"âœ… {unnamed_count} nÅ“uds sans nom ont Ã©tÃ© renommÃ©s.")
    
    # === 5. Sauvegarde du modÃ¨le final ===
    final_model_path = "models/cnn_gesture_brevitas_int8_FINAL_FOR_HLS.onnx"
    model_channels_last.save(final_model_path)
    print(f"ðŸŽ‰ ModÃ¨le final sauvegardÃ© : {final_model_path}")

except Exception as e:
    print(f"âŒ Ã‰chec de la transformation QONNX : {e}")
    print("Sauvegarde du modÃ¨le 'nettoyÃ©' pour dÃ©bogage...")
    cleaned_model.save("models/cnn_gesture_brevitas_int8_CLEANED_DEBUG.onnx")
    raise