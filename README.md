# üß† Acc√©l√©rateur FPGA pour la Reconnaissance de Gestes en Temps R√©el

Ce projet d√©montre un **workflow complet de co-design mat√©riel/logiciel** pour acc√©l√©rer un r√©seau de neurones convolutifs (CNN) de reconnaissance de gestes de la main sur une plateforme embarqu√©e Xilinx Zynq UltraScale+ MPSoC (Avnet UltraZed-EG).

L'objectif est de d√©charger l'inf√©rence du CPU (ARM) vers le FPGA (PL) pour obtenir une **inf√©rence √† faible latence et √©conome en √©nergie**, adapt√©e aux applications embarqu√©es (robotique, appareils intelligents, IHM).

---

## üöÄ Architecture et Flux de Travail

Ce projet ne se contente pas d'entra√Æner un mod√®le ; il le compile en mat√©riel. Le flux de travail complet, de Keras √† un bitstream FPGA, est le suivant :

1.  **Entra√Ænement Keras (Float) :** Un CNN compact est d'abord entra√Æn√© avec Keras en `float32` pour √©tablir une baseline de pr√©cision.
2.  **Entra√Ænement QAT (QKeras) :** Le mod√®le est converti en **QKeras** et r√©-entra√Æn√© (ou *fine-tun√©*) en **Quantization-Aware Training (QAT)**. Cela adapte les poids du r√©seau √† une arithm√©tique de faible pr√©cision (ex: `INT8`) que le FPGA peut calculer efficacement.
3.  **Export `.h5` :** Le mod√®le quantifi√© est sauvegard√© au format `.h5`. **HLS4ML** est capable de lire ce fichier et d'interpr√©ter directement les couches QKeras pour en d√©duire les types de donn√©es mat√©riels (ex: `ap_fixed<8,2>`).
4.  **Synth√®se HLS (HLS4ML) :** **HLS4ML** est utilis√© pour convertir le graphe `.h5` en code **C++ HLS** optimis√©. Il g√©n√®re un projet complet pr√™t pour la synth√®se.
5.  **Compilation Mat√©rielle (Vitis HLS) :** **Vitis HLS** (appel√© par le script de *build* HLS4ML) synth√©tise le C++ en un bloc **IP mat√©riel** (RTL - Verilog/VHDL) pr√™t √† √™tre import√© dans un design logique.
6.  **Int√©gration Syst√®me (Vivado) :** L'IP mat√©riel est import√© dans **Vivado** et int√©gr√© dans un *Block Design* Zynq MPSoC. Il est connect√© au processeur (PS) via une interface **AXI-Lite** (pour le contr√¥le) et √† la m√©moire DDR via **AXI DMA** (pour le flux des pixels d'images). Un *bitstream* est alors g√©n√©r√©.
7.  **D√©ploiement (PYNQ) :** L'application finale s'ex√©cute sous **PYNQ** (Python sur Zynq) sur la carte **UltraZed-EG**. Le code Python (s'ex√©cutant sur le CPU ARM) g√®re :
    * La capture vid√©o via OpenCV.
    * Le pr√©-traitement de l'image (redimensionnement 64x64, normalisation).
    * L'envoi de l'image √† l'acc√©l√©rateur (PL) via le DMA.
    * La r√©ception des r√©sultats (logits) du PL.
    * Le post-traitement (Softmax en CPU) et l'affichage du geste reconnu.